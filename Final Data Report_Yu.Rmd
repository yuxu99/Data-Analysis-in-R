---
title: "Final Data Report"
author: "Yu Xu"
date: "2023-04-05"
output: 
  pdf_document: 
    keep_tex: yes
    fig_width: 3
    fig_height: 2.5
    toc: yes
    fig_caption: yes
    number_sections: yes
header-includes:
  - \usepackage{hyperref}
---

\begin{center}
\href{https://github.com/yuxu99/Data-Analysis-in-R}{Github link: https://github.com/yuxu99/Data-Analysis-in-R}
\end{center}

\newpage

```{r setup, include=FALSE}
library(rattle)
library(tidyverse)
library(caret)
library(ranger)
library(Hmisc)
library(knitr)
library(kableExtra)
library(xtable)
library(pdp)
library(modelsummary)
library(knitr)
opts_chunk$set(echo = TRUE, Emessage = FALSE, warning = FALSE)
```

# Executive Summary

The project used the data in **Shanghai** from Airbnb's website to explore the factors that influence the company's pricing strategy for the apartments or houses they operate, and how to make price predictions based on available information.

## Raw Data Cleaning
* Drop unnecessary variables and symbols.
* Process data for analysis, like generating dummy variables, adjusting data values, etc.
* Classify useful data into numerical, categorical, and dummy variables.
* Deal with missing values.

## Data Visualizaton
* Price and ln_price distributions
* Correlations between price and other available variables
* Price mapping

## Data Prediction 
* Create train and holdout samples
* Data prediction - random forest
* Model diagnostics

The project is organized as follows. Section 2 describes the Airbnb raw data set in Shanghai, Section 3 indicates how to clean the raw data set, Section 4 shows the correlations between price and other available variables through visualization, Section 5 presents the price prediction using random forest model.

\newpage

# Raw Data Description

The data for this project is from [**Airbnb website**](http://insideairbnb.com/get-the-data.html).[^1] And I choose Shanghai as the subject of my analysis since I am more familiar with the regional distribution of Shanghai and the usual prices of apartment hotels, etc. in China, this helps me explore the interpretation of the results of the subsequent analysis.

[^1]: I did not use the Number, Shareholding and Compensation of Executives data mentioned in my previous proposal because I could not find other data for the problems I wanted to study, and the information provided in the data was relatively brief, for example, it did not explain the different definitions of director, manager and supervisor, so there was less basis to rely on in processing the data.

```{r, include=FALSE}
# Clear memory
  rm(list=ls())

# Locations of folders
  data_folder <<- "Data/"
  output_folder <<- "Output/"

# Import raw data
  data <- read.csv(paste0(data_folder, "listings.csv"),
                   sep=",", header = TRUE, stringsAsFactors = FALSE)
```

There are 29423 observations and 74 variables in the raw data set. And it provides useful factors in determining the price:[^2]

[^2]: I only list some data that I personally find useful and interesting, but it actually has a lot of other detailed data, and for a specific introduction to the data variables, please click on the [**link**](https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit#gid=1322284596).

* Basic variables: the number of accommodations, the number of beds, the number of bathrooms, days since the first review, property types, room types and neighborhood categories. These are the variables that people consider first when choosing an apartment based on their needs.
* Review variables: the number of reviews, the rating scores, etc. These variables are important criteria for people to make apartment selection comparisons.
* Dummy variables for amenities: a series of dummy variables, including the availability of Wi-Fi, air conditioning, kitchen, etc.
* Latitude and longitude variables which can be used in mapping data.

However, the raw data is somewhat messy to a certain extent:

* There are some Chinese words that are painful for people who don't know Chinese.
* The variable amenities is a list for each observation, so it needs to be transformed.
* There exist a number of variables with symbolic or null values which also need to be transformed.
* Missing values issues.

So in the next section, I will detail how I clean the raw data.

\newpage

# Process of Cleaning Data

The following are the steps I took with the raw data:

* 1. Drop unnecessary variables like host_thumbnail_url, name, host_about, neighborhood_overview, etc. since they are not so meaningful in determining the price and some of them have overlapping information with other cleaner variables.
* 2. Remove symbols, space or Chinese words for variables used latte like % in rating, $ in price, Chinese words in neighbourhood_cleansed, etc.
* 3. Deal with amenities variable and generate corresponding dummy variables for amenity in those list to indicate whether there is this amenity or not for each observation. The amenities dummy variables are created for prediction analysis since generally the more expensive the apartment or house is, the better equipped it will be especially for the expensive amenity.
* 4. Deal with bathrooms variable and extract the number in the bathrooms_text into bathrooms.
* 5. Create numerical variables (beginning with n_) for existing variables (accommodates, bathrooms, review_scores_rating, etc.) and days since first review which may be treated as the establishing day.
* 6. Create factor (categorical) variables (beginning with f_) for existing variables (room_type[^3], property_type, neighbourhood_cleansed[^4]).Then pool accommodations with 0,1,2,10 bathrooms, pool number of reviews to 3 categories (none, 1-51 and >51), and pool and categorize the number of minimum nights (1, 2, 3, 3+).
* 7. Create amenities dummy variables (use distinguishable new names beginning with d_)
* 8. Keep columns if contain d_, n_,f_, p_, usd_ and some useful variables like longitude, latitude, etc. So the above classification is to make the new data set cleaner.
* 9. Deal with missing values. Replace missing values in terms of domain knowledge for some variables such as assuming there is at least 1 bathroom and 1 bed. Replace missing variables with zeros for review variables and add flag variables which indicate whether it is revised to zero or not, and it can be useful in predication part.

```{r, include=FALSE}
# Import clean data
  data <- read.csv(paste0(data_folder, "airbnb_shanghai_cleaned.csv"),
                     sep = ",", header = TRUE, row.names = 1)
```

```{r, echo = TRUE}
# Where do we have missing variables now?
  to_filter <- sapply(data, function(x) sum(is.na(x)))
  to_filter[to_filter > 0]
```


After cleaning the raw data, there are 29421 observations and 284 variables with no missing value in the clean data set.

In the next section, I will use clean data set to process data visualizations which can present the correlations between price and other variables and give us an overview about all variables.

[^3]: I drop the "Hotel room" category in `room_type` since there are only 2 observations.

[^4]: I think `property_type` does not do a good job of unifying, there is a lot of overlap between many different types, and the explanation of it in the official website is not detailed. But I still keep this variable as a certain reference.

\newpage

# Data Visulization

First, let us look at the distributions of price, Figure 1[^5].

[^5]: I encounter one issue with figure caption after some figures and try to fix it but fail. So I just give up figure caption.

```{r, echo = FALSE, fig.align = "center"}
  summary(data$price)

  ggplot(data, aes(x = price)) +
    geom_histogram(aes(y = after_stat(density)), bins = 30, color = "white", fill = "steelblue") +
    geom_density(alpha = .2, fill = "#FF6666") +
    theme_minimal() +
    labs(x = "Price", y = "Density")
```

From Figure 1, we can see that the price variable has some extreme values. So for a more efficient analysis, we restrict the price (in $) to a more reasonable range (price < 1000). And then look at the distributions of price agin, Figure 2.

```{r, echo = FALSE, fig.align = "center"}
# Filter price for a reasonable range (price < 1000)
  data <- data |>
    filter(price < 1000)

# Histograms of price
  ggplot(data, aes(x = price)) +
    geom_histogram(aes(y = after_stat(density)), bins = 30, color = "white", fill = "steelblue") +
    geom_density(alpha = .2, fill = "#FF6666") +
    theme_minimal() +
    labs(x = "Price", y = "Density")
```

Then let us look at as well as analyze the more interesting figures.

* 1. Heatmap of correlations between numerical variables, Figure 3.

```{r, echo = FALSE, fig.height = 3, fig.width = 5, fig.align = "center"}
  numeric_data <- data |>
      select(n_accommodates, n_bathrooms, n_review_scores_rating, n_number_of_reviews, n_reviews_per_month, n_minimum_nights, n_beds, n_days_since, price)
    
  cor_matrix <- cor(numeric_data)
  cor_matrix <- cor_matrix |>
    as.data.frame() |>
    rownames_to_column(var = "Var1") |>
    pivot_longer(-Var1, names_to = "Var2", values_to = "Correlation")
  
  ggplot(cor_matrix, aes(x = Var1, y = Var2, fill = Correlation)) +
    geom_tile() +
    scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```

We can see that the number of accommodates has the highest correlation with price, but it is surprising that review scores rating has a lower correlation with price.

* 2. Scatter plot of price against n_review_scores_rating, Figure 4.

```{r, echo = FALSE, fig.height = 3, fig.width = 6, fig.align = "center"}
  ggplot(
      data, 
      aes(x = n_review_scores_rating, y = price, color = f_room_type)
    ) + geom_point() +
      geom_smooth(method = 'gam', formula = y ~ s(x, bs = "cs"), se = FALSE) +
      labs(x = "n_review_scores_rating", y = "price", color = "f_room_type")
```

Most of the n_review_scores_rating are between 4 and 5. For Entire home/apt and private rooms, there is still a significant up and down price fluctuation between 4 and 5 ratings, but overall there is no positive or negative correlation for all room types.I think location plays a key role in this. For example, a house in the center of the city may be smaller and have fewer amenities, giving the lodger a poorer experience, but its prime location is enough to keep the house in high demand and thus the price will not be low.

* 3. Scatter plot of price against n_accommodates for each region, Figure 5.

```{r, echo = FALSE, fig.height = 3, fig.width = 7, fig.align = "center"}
# Calculate average price and n_accommodates for each region
  neighbourhood_avg <- data |>
    group_by(f_neighbourhood_cleansed) |>
    summarise(avg_price = mean(price),
              avg_accommodates = mean(n_accommodates))
  
  # Create scatter plot with different colours for different room types
  ggplot(data, aes(x = n_accommodates, y = price, color = f_room_type)) +
    geom_point() +
    facet_wrap(~f_neighbourhood_cleansed, nrow = 3) +
    geom_hline(aes(yintercept = avg_price), data = neighbourhood_avg, 
               linetype = "dashed", color = "black") +
    geom_vline(aes(xintercept = avg_accommodates), data = neighbourhood_avg, 
               linetype = "dashed", color = "black") +
    labs(x = "n_accommodates", y = "price", color = "f_room_type") +
    theme(legend.position = "bottom", legend.text = element_text(size = 6), 
        strip.text.x = element_text(size = 8, face = "bold")) +
    guides(color = guide_legend(ncol = 3)) +
    scale_color_brewer(palette = "Set1")
```

This figure provides a lot of useful information. First, we can see the difference in average price between the different neighborhoods, with ChongmingDistrict (more private rooms) being the highest. Second, Entire home/apt generally has more accommodations and higher prices. Finally, another strange phenomenon is that Shared room can accommodate more people but the price is lower compared to private.

* 4. Scatter plot of longitude and latitude colored by price, Figure 6

```{r, echo = TRUE, fig.height = 4, fig.width = 7, fig.align = "center"}
  ggplot(data, aes(x = longitude, y = latitude, color = price, shape = f_neighbourhood_cleansed)) +
      geom_point(size = 3) +
      scale_color_gradient(low = "blue", high = "red") +
      scale_shape_manual(values = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 17, 19, 22, 23, 25)) +
      theme_minimal() +
      guides(shape = guide_legend(title = "Neighbourhood")) +
      theme(legend.key.size = unit(0.3, "cm"))
```

This figure is my favorite. And I would like to explain what each step does in the codes:

* Initialize a new ggplot object, specifying the data to use and the mapping of variables to aesthetics. In this case, the x aesthetic is mapped to longitude, the y aesthetic is mapped to latitude, the color aesthetic is mapped to price, and the shape aesthetic is mapped to f_neighbourhood_cleansed.
* Add a geom_point layer to the plot, which creates a scatterplot of the longitude and latitude coordinates, with the point size set to 3.
* Set the color scale for the price variable to a gradient from blue to red.
* Manually set the shape scale for the f_neighbourhood_cleansed variable to a vector of integers ranging from 1 to 25.
* Apply the theme_minimal theme to the plot, which removes all background elements except the data and axis labels.
* Add a legend for the shape aesthetic, with the title "Neighbourhood".
* Set the size of the legend keys to 0.3 cm using the theme function.

Thanks to the longitude and latitude variables in the data, I am able to draw the distribution of price in the form of a map. First, we can see the general distribution of apartments and houses, which are basically concentrated in the center of Shanghai. Second, the overall darker color of prices in ChongmingDistrict in the upper right corner is consistent with our conclusion above.

\newpage

# Data Prediction [^6]

[^6]: I have previously used Random Forest models via Python in a machine learning course. I studied the codes of R from Professor Gabor Bekes' book [Data Analysis for Business, Economics, and Policy](https://gabors-data-analysis.com/) to complete this project.

## Data Processing for Prediction

For a more efficient predictive analysis, I focus on normal apartments with n_accommodates smaller than 8.

```{r, echo = FALSE}
# Focus on normal apartments, n_accommodates < 8
  data <- data |>
    filter(n_accommodates < 8)
```

## Create Train and Holdout Samples

Randomly select 70% of the data as the training sample and the training sample is where we do everything, including cross-validation. Holdout samples is the sample used for the final test of the model.

```{r, include=FALSE}
  set.seed(2801)

  train_indices <- as.integer(createDataPartition(data$price, p = 0.7, list = FALSE))
  data_train <- data[train_indices, ]
  data_holdout <- data[-train_indices, ]
    
  dim(data_train)
  dim(data_holdout)
```

## Data Prediction - Random Forest

* Predictors defining

  Predictors = Basic variables + Reviews + Amenities dummy variables

```{r, include=FALSE}
# Basic variables
  basic_vars <- c(
    "n_accommodates", "n_beds", "n_days_since",
    "f_property_type","f_room_type", "f_bathroom",
    "f_neighbourhood_cleansed")
  
  # Reviews
  reviews <- c("n_number_of_reviews", "flag_n_number_of_reviews", 
               "n_review_scores_rating", "flag_review_scores_rating")
  
  # Dummy variables
  amenities <-  grep("^d_.*", names(data), value = TRUE)
  
  # Predictors needed in Random Forest
  predictors <- c(basic_vars, reviews, amenities)
```

* Random forest model

  Do 5-fold CV (cross validation), Set tuning for random forest model, and run random forest model.

  A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.

  A random forest needs a little tuning, i.e. three parameters: the number of bootstrap samples, the number of variables considered for each split, and the minimum number of observations in the terminal nodes of each tree as a stopping rule. For the number of bootstrap draws, I go with the default option of 500. For the number of variables, one rule of thumb is to pick the square root of the total number of variables, which would be around 10, so I try 8, 10, and 12. For the minimum number of observations in the terminal nodes, I choose 5, 10, and 15.

```{r, include=FALSE}
# Do 5-fold CV
  train_control <- trainControl(method = "cv",
                                number = 5,
                                verboseIter = FALSE)
  
  # Set tuning for random forest model
  tune_grid <- expand.grid(
    .mtry = c(8, 10, 12),
    .splitrule = "variance",
    .min.node.size = c(5, 10, 15)
  )
  
  # Random forest model
  set.seed(1234)
  system.time({
    rf_model <- train(
      formula(paste0("price ~", paste0(predictors, collapse = " + "))),
      data = data_train,
      method = "ranger",
      trControl = train_control,
      tuneGrid = tune_grid,
      importance = "impurity"
    )
  })
  rf_model
```

* Evaluate random forest model

```{r, echo = FALSE}
  rf_tuning_model <- rf_model$results |>
    dplyr::select(mtry, min.node.size, RMSE) |>
    dplyr::rename(nodes = min.node.size) |>
    spread(key = mtry, value = RMSE)
  rf_tuning_model
  
  # Convert the table to kable format
  rf_tuning_model_table <- kable(rf_tuning_model, format = "html", caption = "Random Forest Model Tuning Results") |>
    kable_styling(bootstrap_options = c("striped", "hover"), full_width = F) |>
    column_spec(1, bold = T)
```

  We can find that the RMSE is the smallest with max_features of 12 and min_samples_leaf of 5, which is 153.1678.
  
\newpage

## Model Diagnostics

* Variable importance plots

a) Full variable importance plot, top 10 only, Figure 7
  
```{r, echo = FALSE, fig.height = 3, fig.width = 6, fig.align = "center"}
 # First need a function to calculate grouped varimp
  group.importance <- function(rf.obj, groups) {
    var.imp <- as.matrix(sapply(groups, function(g) {
      sum(importance(rf.obj)[g], na.rm = TRUE)
    }))
    colnames(var.imp) <- "MeanDecreaseGini"
    return(var.imp)
  }

  # Variable importance plots
  rf_model_var_imp <- importance(rf_model$finalModel)/1000
  rf_model_var_imp_df <-
    data.frame(varname = names(rf_model_var_imp),imp = rf_model_var_imp) |>
    mutate(varname = gsub("f_neighbourhood_cleansed", "Borough:", varname) ) |>
    mutate(varname = gsub("f_room_type", "Room type:", varname) ) |>
    arrange(desc(imp)) |>
    mutate(imp_percentage = imp/sum(imp))
  
  # Full variable importance plot, top 10 only
  rf_model_var_imp_plot <- ggplot(rf_model_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
    geom_point(color=colors()[37], size=1) +
    geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color=colors()[37], size=0.75) +
    ylab("Importance (Percent)") +
    xlab("Variable Name") +
    coord_flip() +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    theme_bw() +
    theme(axis.text.x = element_text(size=4), axis.text.y = element_text(size=4),
          axis.title.x = element_text(size=4), axis.title.y = element_text(size=4))
  rf_model_var_imp_plot
```

This is a graph of the importance of all the variables in the top 10, from which we are able to see that the most important variables for prediction are the number of people it can accommodate, the number of beds, days since the first review,the number of reviews, the amenity dryer, the amenity buildingstaff.

b) Variable importance plot grouped, Figure 8

Another way is to group qualitative (factor) variables that are entered as several binary variables. Variable importance then is a sum of gains (in terms of MSE reduction) by splits involving the factor variable. For instance, rather than considering splits by the borough binary variable, we now consider all the splits by any values of the neighborhood variable.
  
```{r, echo = FALSE, fig.height = 3, fig.width = 6, fig.align = "center"}
#Variable importance plot grouped
  varnames <- rf_model$finalModel$xNames
    f_neighbourhood_cleansed_varnames <- grep("f_neighbourhood_cleansed",varnames, value = TRUE)
    f_property_type_varnames <- grep("f_property_type",varnames, value = TRUE)
    f_room_type_varnames <- grep("f_room_type",varnames, value = TRUE)
    
    groups <- list(f_neighbourhood_cleansed=f_neighbourhood_cleansed_varnames,
                   f_property_type = f_property_type_varnames,
                   f_room_type = f_room_type_varnames,
                   f_bathroom = "f_bathroom",
                   n_days_since = "n_days_since",
                   n_accommodates = "n_accommodates",
                   n_beds = "n_beds")
    
  rf_model_var_imp_grouped <- group.importance(rf_model$finalModel, groups)
  rf_model_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_var_imp_grouped),
                                              imp = rf_model_var_imp_grouped[,1])  |>
    mutate(imp_percentage = imp/sum(imp))
  
  rf_model_var_imp_grouped_plot <-
    ggplot(rf_model_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
    geom_point(color=colors()[37], size=1) +
    geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color=colors()[37], size=0.7) +
    ylab("Importance (Percent)") +   xlab("Variable Name") +
    coord_flip() +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    theme_bw() +
    theme(axis.text.x = element_text(size=4), axis.text.y = element_text(size=4),
          axis.title.x = element_text(size=4), axis.title.y = element_text(size=4))
  rf_model_var_imp_grouped_plot
```

From this figure, we can find that property type becomes the most important variable for prediction, followed by the neighbourhood category variable. The higher variable importance of neighbourhood category variable is reasonable because the price difference between different areas of Shanghai is quite large, and the central areas located in Huangpu, Jing'an, Changning and Xuhui districts tend to be more expensive. However, when we look at this area variable separately, it also becomes less important due to the higher similarity of the adjacent areas. The reason is similar for property type.

* Partial Dependence Plots

Partial Dependence Plots show how average y differs for different values of x when all the other x values are the same. Partial Dependence Plots can also be used to study the heterogeneity if the predictor is category variable. The following is Figure 9 of partial dependence plots for number of accommodates.[^7]
  
```{r, echo = FALSE, fig.height = 3, fig.width = 6, fig.align = "center"}
 pdp_n_acc <- pdp::partial(rf_model, pred.var = "n_accommodates", pred.grid = distinct_(data_holdout, "n_accommodates"), train = data_train)
  pdp_n_acc_plot <- pdp_n_acc |>
    autoplot(geom_line(stat="identity") ) +
    geom_point(color=colors()[37], size=2) +
    geom_line(color=colors()[37], size=1) +
    ylab("Predicted price") +
    xlab("Accommodates (persons)") +
    scale_x_continuous(limit=c(1,7), breaks=seq(1,7,1))+
    theme_bw()
  pdp_n_acc_plot
```

The partial dependence plot of the number of accommodations shows that as the number of accommodations gets higher, the price increases accordingly.

[^7]: I also have partial dependence plot for roomtype category variable in the code files. But the result has a warning and I don't know how to fix it, so I don't put it in the report in order to keep it neat. That figure shows the price of Entire home/apt is the highest and the price of Shared room is the lowest, which is consistent with the conclusion we draw in the data visualization part.